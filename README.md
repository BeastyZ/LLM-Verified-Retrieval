# LLM-Verified Retrieval for Verifiable Text Generation
This repository contains the code and data for paper LLatrieval: LLM-Verified Retrieval for Verifiable Generation. This repository also includes code to reproduce the method we propose in our paper.


## Quick Links
- [Requirements](#requirements)
- [Data](#data)
- [Code Structure](#code-structure)
- [Reproduce Our Method](#reproduce-our-method)
- [Citation](#citation)


## Requirements
1. We recommend that you use the python virtual environment and then install the dependencies.
    ```
    conda create -n lvr python=3.9.7
    ```
2. Next, activate the python virtual environment you just created.
    ```
    conda activate lvr
    ```
3. Finally, before running the code, make sure you have set up the environment and installed the required packages.
    ```
    pip install -r requirements.txt
    ```

## Data
We uploaded the data to [Hugging Face](https://huggingface.co/datasets/BeastyZ/LLM-Verified-Retrieval)ðŸ¤—. 

**Start by installing ðŸ¤— Datasets:**
```
pip install datasets
```

**Load a dataset**
```
from datasets import load_dataset

dataset = load_dataset("BeastyZ/LLM-Verified-Retrieval")
```
After downloading the data, you need to manually specify the data path. This may be a little difficult, so we recommend that you manually download the data from huaggingface to your current working directory.

**NOTE**

For the Sphere and Wikipedia snapshot corpora, please refer to [ALCE](https://github.com/princeton-nlp/ALCE) for more information.


## Code Structure
* `commands/`: folder that contains all shell files.
* `llm_retrieval_prompt_drafts/`: folder that contains all prompt files.
* `llm_retrieval_related/`: folder that contains code for iteratively selecting supporting documents.
* `multi_process/`: folder that contains code for BM25 retrieval with multi-process support.
* `openai_account_files/`: folder that contains all OpenAI account files.
* `prompts/`: folder that contains all instruction and demonstration files.
* `eval.py`: eval file to evaluate generations.
* `filter.py`: filter file to filter bad samples.
* `gen_used_field.py`: code for generating summary or answer.
* `Iterative_retrieval.py`: code for reproduce our method.
* `openai_account_manager.py`: code for managing OpenAI accounts.
* `run.py`: run file to generate citations.


## Reproduce Our Method
**NOTE:** There must be raw data and a corpus for retrieval before running the following commands. Once you have them, you also need to modify the parameters of the corresponding files in the `commands` directory.

For ASQA, use the following command
```bash
bash commands/asqa_iterative_retrieval.sh
```

For QAMPARI, use the following command
```bash
bash commands/qampari_iterative_retrieval.sh
```

For ELI5, use the following command
```bash
bash commands/eli5_iterative_retrieval.sh
```

The result will be saved in `iter_retrieval_50/`.


## Citation
Add the relevant information later
